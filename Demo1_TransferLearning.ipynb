{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfff6383",
      "metadata": {
        "id": "bfff6383"
      },
      "source": [
        "# Translation from English to Spanish using Flan-T5 and Helsinki-NLP/opus-100 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f128b9",
      "metadata": {
        "id": "e9f128b9"
      },
      "source": [
        "\n",
        "## Introduction\n",
        "In this notebook, we will use the Flan-T5 model to perform translation from English to Spanish. We will use the \"Helsinki-NLP/opus-100\" dataset from Hugging Face, specifically the en-es subset, to train and evaluate our translation model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7152441a",
      "metadata": {
        "id": "7152441a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832110977,
          "user_tz": 420,
          "elapsed": 11943,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "686e8df5-277d-4136-c7cd-b721ad305e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8885d52e",
      "metadata": {
        "id": "8885d52e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "ee2b0faa352b4501a8a3588051a369ed",
            "bddea91829f941b8bd587f27a2e0363f",
            "630f0722d0454df598e7a449eb28cfc9",
            "2f375d8dc19c46b69bc4fa12e5fc52ce",
            "3dc092c3da794a689dce37c9b45f6a00",
            "a0a22d9eb7234bd6a89f377f8deabc5d",
            "98e156adb49b46fa8d66f13d26fbd878",
            "bc6934e2581344b7b3cbe6e43f37ceaa",
            "4eed7248b6474616b0127b458e8ae420",
            "04a52de53667410cb92e8357739a6796",
            "bb387f9c710a43bcbc40fd8c409e5ac6",
            "beaff41fddb749db9be520534ec82dcc",
            "afeb5518c1d944e690ad292d579d8c3b",
            "65e9c3d4dcad43a5b2102f504b8137b1",
            "8f1d11840f9448188211379750bc9131",
            "7085473facb0482787f02d9486a6d14c",
            "cab4bd779c164f55bd79aec91207a4fd",
            "01d6a49d973941e089fe64f846e1d19b",
            "b8d0b71c725e455bb982150e0ac76d66",
            "1cda320bb6e44142bdb1f1588e9de83b",
            "06cae339ab7a497486f4d93aeeb3e011",
            "a3766d7d859a4f93bd2b2767e848803f",
            "211efa01318d414e92632cee808c86f2",
            "3ac32da7f2914e4b80972a46a33f557b",
            "7f47227c163b442a8dabf475349a52b1",
            "615ab83e5dbd457c9bc01983ede8aabb",
            "dd1c24818bd44be7b7829c8a2fc2571b",
            "466b0e3ab6a34f319c9b1168f60b2943",
            "eeb7888d63e04d068f081dc965b9e5fd",
            "db88f59dfe4b4c379845d9a86266922d",
            "84b26300cb1845d99e24056578bb9ce1",
            "85d0e1bb96314463a81a456f95e2438a",
            "461c1e40a16f4727acec1ff4bc3ac083",
            "f7174bbc0c7d4b079d860bdc9c87aa77",
            "260416ff4bdf4debbf19acd170c8357d",
            "d02af3014bcf46b4a861c638c90d55e4",
            "edc9ad3ca5164acd813743a41576d915",
            "294f683860e44939ab25c8ed814dea3d",
            "5a13b12ec92f453fa34ead416c316060",
            "edc372b06f4d44c4aa80ed3e0c7c0124",
            "1c10394d5b9b4740a9c9cd6a673c08e9",
            "e5d4446d7e8644518572cca8a9c59cef",
            "a691b36f6fbc4cd6a6307bb06365b327",
            "3a3fddc755124c83b4276668547f9865",
            "53580ff8c3ba42e5847e1ae2a7feaf6a",
            "3aafb4af5f1d49acbd932bc2b49afaac",
            "fda248c904b148878fb729c4052523c1",
            "ae17612912cd4dc78a0f9529faab51ed",
            "8b6acf0a2609481f93f272ec887c824a",
            "991e42bc60f84283a8681fee2e9956ba",
            "2831c58dd6704b0b861b8e770ed7fc03",
            "73f532ac51344b168568be52bdfc3df6",
            "f2387fb325cd4e7d950fba0b9c665629",
            "363cda54aac64e789dc0db6870562032",
            "f5c8bd54a8cf499a83d5573552dfde4d",
            "079caf7ae1a74fb3a9c4aa53ace6bb08",
            "19b0b01715264f919dfdbe01c2aad336",
            "81c50c80da0b42d6870e742b0b761eb6",
            "3f855b1e5cde4bf8ab5b15631fae9801",
            "3c46d048016046a1bd122f9cb408e45b",
            "bc0c084e33e14ad8a5e96b956f60ada6",
            "5d773d2d2a524725963087b1d47b80fc",
            "8c312e31df4244daaf13e473a256da9c",
            "e83b5b9dbcd343c3a715bbd107f2888d",
            "ba7d2dfc474449db92265605a228b95d",
            "a954bd635a604c0b908bd53a41e79810"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832167689,
          "user_tz": 420,
          "elapsed": 18730,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "164e44e3-8de5-4036-90d9-1877cfe40d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee2b0faa352b4501a8a3588051a369ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beaff41fddb749db9be520534ec82dcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211efa01318d414e92632cee808c86f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7174bbc0c7d4b079d860bdc9c87aa77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53580ff8c3ba42e5847e1ae2a7feaf6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "079caf7ae1a74fb3a9c4aa53ace6bb08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e985d020",
      "metadata": {
        "id": "e985d020"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5e3f06d1",
      "metadata": {
        "id": "5e3f06d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "937eaff4e447466984862d3fb849f650",
            "c78dcb89037f44fcb7e346d72d095a1b",
            "11eca73dd7284e908251ae6300afbc8a",
            "dbfd50a256b0421fb0265dcff683ddea",
            "4b70a0f1c12e4025845bbc45d95d6e0a",
            "33aa7e69449c47aea4c96662e6530725",
            "e906c12c95a04bdc929e1a00d7af2cd6",
            "b6b31a775e944b08bdffb38b529389a7",
            "b7470418aff543deb6b8ea9f979e995f",
            "6e76c2462ddf4e718d3207da4a1c7b54",
            "ca0009b1c1aa4a57b0ff8ebf3699b293",
            "fd27ff49d3544f2abc6dbaac1d89a7c5",
            "835b011684d945c2bb3ce089afb55cfa",
            "8b335a81a11e49b6956e0c265cb76050",
            "92cad18d9a724ced9368182d3ffa3a42",
            "d8303923875c456b80fdd12e9d12469b",
            "2f4410d345e7410482aeedc6b998b110",
            "82c7991013184d229da9eb9ac3a365fd",
            "55e8a1b291d94079a4f8c64c700bec55",
            "f68f6d94671e472f8f9ca66f9ed0b02f",
            "93acfa381cc54135a41942a2f97f9925",
            "50d6dff5f271419d90bc45653b03e8f7",
            "da7bb9665b8d46a79fa63f5712f28668",
            "1ac156d263ba4fdfb07d05ea14207d51",
            "a1772662db1f4541b9039fa93ed8d151",
            "c616f478415e40c2b3c6516fee16875f",
            "c27cdaf5ef5d49f091abc63224ff0737",
            "e8c6b377f55b4c4abb6b0772b2ef436b",
            "ca56a57a22264de6a96777fc648d659e",
            "e403e766ec5549b3962b446e2ba0f303",
            "31789d8c06814be884434034cc9f0c7e",
            "4d4a2d390bed4d57a920a2b80f1f0e34",
            "b619a71c38bb4179a3844fb97aa9651f",
            "d96c39d5785e4001b7a22eddb72eee48",
            "9bd3df5d96ce411c919ba65cb5967485",
            "2445cfb289b34ea9a4d0b228b708c558",
            "5a86663be96d4d84b155ff961ca044d2",
            "6712c461b94042338f7a3af80a22cdd3",
            "94ee11b406b94d019614670457c7c23a",
            "e0769f6b5a324e7781b84aa8b05f74b8",
            "31d197c8c6cc43909b5a047ecd0650ef",
            "25d2540ccb8b47ec8439b2c6223b26cf",
            "4012fc49edd2431480866347ddcb5110",
            "ae35275da91041d9899548c4c76d1263",
            "8e7cfd78839a4d46bec73ff5cf8d41a3",
            "36fa7597a2ef49a0906b52801bae6490",
            "f47d7e281c784cb690d85ceb74c699c0",
            "1a43f517d1824451b7b309bcd61e1fff",
            "ca166746aff541d184990b33a6bbc6cf",
            "64f754e70b6a4ece9f2b6203357443e2",
            "dc912fbb660a4933a9f9a921e684994f",
            "490fd89085064a9f9037944b1f2aa3b3",
            "f8f7726fe12b481d9859f3e58bb07c56",
            "46c445f7960a4f3497ba9eaee0955ca1",
            "561aecc301a742c183473f4ee5cf39dd",
            "a7dde037688b48ea9fc07948123d00d1",
            "a4aa454d6a7b433a848503432910dac7",
            "1f675b0471214a8a9c9f6a90c1e8396a",
            "a1ae8b66c1ab4f4faf573cb3cfba0eb7",
            "78ce9d276b8940a9b588c90e23f0861c",
            "4aeafa6fe3cf466dac2f4d32e21fd888",
            "5fca91f81295401195a53b109208085f",
            "c7c6c3a718604ed6b4ace7f0535d0e80",
            "defd6a2daa894769b5caab49a8ad7365",
            "d8e7bc29c41a494fbc431cb5ca54d8c7",
            "3d736d461e17407ca82ed7a48f6d972c",
            "99c25e504e8241b1bb7b829c130ae0c8",
            "fd44a8dcec924a40a1553da176ff94ee",
            "c1f95dd73cab47f29bbe3dd4567c2290",
            "81fd3b017f4c41799770d73ab70483a8",
            "fe813e4c92a540ff9d071583478c1c0e",
            "73b718ff0b3145c6963588b369e85960",
            "a1c5256ad49a4d6aae5ca569fc86a2aa",
            "d7d427cc90634207a0433ddc291536fd",
            "79d024acb3db4274a8d5e6d254f3a8f3",
            "3d51492af2d84aa8b53512446fc99838",
            "f3d13f54e6f44a65a72810ce74519243"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832217308,
          "user_tz": 420,
          "elapsed": 10250,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "73fa3beb-efb5-4c06-8f0f-89333e82ba35"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "937eaff4e447466984862d3fb849f650"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/237k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd27ff49d3544f2abc6dbaac1d89a7c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da7bb9665b8d46a79fa63f5712f28668"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/238k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d96c39d5785e4001b7a22eddb72eee48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e7cfd78839a4d46bec73ff5cf8d41a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7dde037688b48ea9fc07948123d00d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99c25e504e8241b1bb7b829c130ae0c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'en': \"It was the asbestos in here, that's what did it!\", 'es': 'Fueron los asbestos aquí. ¡Eso es lo que ocurrió!'}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Helsinki-NLP/opus-100 dataset\n",
        "dataset = load_dataset('Helsinki-NLP/opus-100', 'en-es')\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065dd4f0",
      "metadata": {
        "id": "065dd4f0"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "130d92fb",
      "metadata": {
        "id": "130d92fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "683065d7fea743388bf2c4e1f4e7ed33",
            "cbc398c4e98b48aab906fc85a8791c5c",
            "c2afea375d354854a9092219ae882ec1",
            "ad6f797566364bc3b64fd1e13eb5067c",
            "b18a61b9c45c45d09e00f7b1a1e7ba6a",
            "5f2e7fdc2bc343789c0fe0a4aa225b6c",
            "f6d37d1fa8d541ff9e79ea5b61091bbb",
            "f1577beda5614b6398504181386e97b9",
            "c6c9178e173a4d64be55b99ecb3ef4ca",
            "5e5e9418dbd64f599fcce65910cea125",
            "21ecaed39d0949f29216ea945ed26b0f",
            "69937e2211e9462e8a6a85ebd3818b47",
            "f9508e8a3b7647149ef64e1bd1c6effb",
            "78f80025cbc94b169a9210b1598de0cb",
            "2ecd19363df94ef79585672c813104dd",
            "0150aacf102645508015a63840d9c997",
            "d4d93281988d4cf7a17f6a48b373d615",
            "fd8d21a05a0b4feb92c7bf233c4b7fe1",
            "1f1d502f1d534ad5b3914eb3e149ca88",
            "cead80ccbe5f44369e8a8d3c2f44f62d",
            "8e5b49d958b7419aa4664508e0182e28",
            "b1c5ed8c7c3443479234991638b2d462"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832398531,
          "user_tz": 420,
          "elapsed": 6135,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "2be262d0-61dd-4689-8d12-f2487c59e0f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "683065d7fea743388bf2c4e1f4e7ed33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69937e2211e9462e8a6a85ebd3818b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'en': \"It was the asbestos in here, that's what did it!\", 'es': 'Fueron los asbestos aquí. ¡Eso es lo que ocurrió!'}, 'input_ids': [30355, 15, 45, 1566, 12, 5093, 10, 94, 47, 8, 23778, 16, 270, 6, 24, 31, 7, 125, 410, 34, 55, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [6343, 49, 106, 10381, 23778, 3, 9, 2436, 2, 5, 3, 2, 427, 7, 32, 3, 15, 7, 6899, 238, 3, 32, 3663, 52, 23, 4922, 55, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [6343, 49, 106, 10381, 23778, 3, 9, 2436, 2, 5, 3, 2, 427, 7, 32, 3, 15, 7, 6899, 238, 3, 32, 3663, 52, 23, 4922, 55, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the dataset for input into the model\n",
        "def preprocess_data(examples):\n",
        "    inputs = [f'Translate from English to Spanish: {example[\"en\"]}' for example in examples['translation']]\n",
        "    targets = [example['es'] for example in examples['translation']]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # For decoder inputs\n",
        "    decoder_inputs = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_dataset = dataset['train'].select(range(30000)).map(preprocess_data, batched=True)\n",
        "test_dataset = dataset['test'].map(preprocess_data, batched=True)\n",
        "\n",
        "print(train_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e84ee3b",
      "metadata": {
        "id": "6e84ee3b"
      },
      "source": [
        "## Converting to TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6d1a76b3",
      "metadata": {
        "id": "6d1a76b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832518083,
          "user_tz": 420,
          "elapsed": 1805,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "a5738784-1bae-4fdc-cdd2-5c8a852ace38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert Hugging Face datasets to TensorFlow datasets\n",
        "train_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
        "    label_cols=['labels'],\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    collate_fn=None\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
        "    label_cols=['labels'],\n",
        "    shuffle=False,\n",
        "    batch_size=64,\n",
        "    collate_fn=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52fd5609",
      "metadata": {
        "id": "52fd5609"
      },
      "source": [
        "## Freezing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "E-zjSYjy0yLn",
      "metadata": {
        "id": "E-zjSYjy0yLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832549812,
          "user_tz": 420,
          "elapsed": 347,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "ccdfb229-a478-48ef-925c-bd494ad040e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            " lm_head (Dense)             multiple                  24674304  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 247577856 (944.43 MB)\n",
            "Trainable params: 247577856 (944.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dc180d66",
      "metadata": {
        "id": "dc180d66",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832591479,
          "user_tz": 420,
          "elapsed": 543,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Freeze the LLM layer\n",
        "model.get_layer(\"shared\").trainable = False\n",
        "model.get_layer(\"encoder\").trainable = False\n",
        "model.get_layer(\"decoder\").trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "sWh-GvlZ0zu_",
      "metadata": {
        "id": "sWh-GvlZ0zu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718832593371,
          "user_tz": 420,
          "elapsed": 4,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "278f8fe2-8ad7-4d0f-e8cb-6d2d76335b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            " lm_head (Dense)             multiple                  24674304  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 247577856 (944.43 MB)\n",
            "Trainable params: 24674304 (94.12 MB)\n",
            "Non-trainable params: 222903552 (850.31 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4d5c6c",
      "metadata": {
        "id": "ab4d5c6c"
      },
      "source": [
        "\n",
        "### Important Considerations in Transfer Learning\n",
        "\n",
        "1. **Freezing the LLM Layer:** In transfer learning, it's important to freeze the pre-trained language model layer to retain the knowledge it has already acquired and to avoid overfitting. This allows the model to leverage its pre-trained capabilities while focusing on learning the new task-specific nuances.\n",
        "\n",
        "2. **Loss Function with `from_logits=True`:** When fine-tuning language models from Hugging Face, it's crucial to use the loss function with `from_logits=True`. This is because these models do not apply softmax to their outputs, and using `from_logits=True` ensures that the loss is computed correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5791b03e",
      "metadata": {
        "id": "5791b03e"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8608ebea",
      "metadata": {
        "id": "8608ebea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718833360140,
          "user_tz": 420,
          "elapsed": 663691,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "ca1a2250-5707-45a1-c135-804aa0d5cc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7cb35701aa70> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7cb35701aa70> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "469/469 [==============================] - 304s 445ms/step - loss: 25.6175 - val_loss: 2.5222\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 179s 383ms/step - loss: 1.4546 - val_loss: 0.8384\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 179s 382ms/step - loss: 0.7756 - val_loss: 0.6219\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7cb2a01e9330>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, validation_data=test_dataset, epochs=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073993dd",
      "metadata": {
        "id": "073993dd"
      },
      "source": [
        "## Performing Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3lfhqcYFsAh",
      "metadata": {
        "id": "C3lfhqcYFsAh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "53933e6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53933e6b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718833702790,
          "user_tz": 420,
          "elapsed": 209731,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "863fe79a-4f54-46ce-cff7-b1da6881e16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Translate from English to Spanish: If your country produced ODS for this purpose, please enter the amount so produced in column 6 on Data Form 3.”\n",
            "Reference Translation: Si su pas produjo SAO para estos usos, srvase anotar en la columna 6 del formulario de datos 3 la cantidad correspondiente”.\n",
            "Translated Text: \n",
            "\n",
            "Input: Translate from English to Spanish: Damn it! Oops.\n",
            "Reference Translation: Maldita sea!\n",
            "Translated Text: \n",
            "\n",
            "Input: Translate from English to Spanish: - Bird, you don't have to be so brave all the time.\n",
            "Reference Translation: - Bird, no tienes que ser tan valiente todo el tiempo.\n",
            "Translated Text: \n",
            "\n",
            "Input: Translate from English to Spanish: I feel cod there.\n",
            "Reference Translation: Me siento bien all.\n",
            "Translated Text: \n",
            "\n",
            "Input: Translate from English to Spanish: We humans will inevitably end up controlling our own evolution, and, because our power is emergent from nature we will make use of this acquired capacity sooner or later, for better or worse.\n",
            "Reference Translation: Nosotros humanos terminaremos inevitablemente controlando nuestra propia evolución y, puesto que nuestro poder emerge de la naturaleza, tarde o temprano utilizaremos esa capacidad adquirida, para bien o para mal.\n",
            "Translated Text: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Perform translation on a few examples from the test set\n",
        "def translate(inputs):\n",
        "    outputs = model.generate(inputs[0][\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Translate and display a few examples\n",
        "for batch in test_dataset.take(5):\n",
        "    translated_text = translate(batch)\n",
        "    print(f\"Input: {tokenizer.decode(batch[0]['input_ids'][0], skip_special_tokens=True)}\")\n",
        "    print(f\"Reference Translation: {tokenizer.decode(batch[1][0], skip_special_tokens=True)}\")\n",
        "    print(f\"Translated Text: {translated_text}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25d96fa",
      "metadata": {
        "id": "b25d96fa"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "In this notebook, we used the Flan-T5 model to perform translation from English to Spanish using the Helsinki-NLP/opus-100 dataset. We preprocessed the dataset, fine-tuned the model while freezing the LLM layer, and performed translations. We manually validated the translations to assess the quality of the model's performance. The results demonstrate the effectiveness of the Flan-T5 model for translation tasks.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
