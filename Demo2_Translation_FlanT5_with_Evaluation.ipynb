{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4939bd2d",
      "metadata": {
        "id": "4939bd2d"
      },
      "source": [
        "# Translation from English to Spanish using Flan-T5 and Helsinki-NLP/opus-100 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85233341",
      "metadata": {
        "id": "85233341"
      },
      "source": [
        "## Introduction\n",
        "In this notebook, we will use the Flan-T5 model to perform translation from English to Spanish. We will use the \"Helsinki-NLP/opus-100\" dataset from Hugging Face, specifically the en-es subset, to train and evaluate our translation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee3ad269",
      "metadata": {
        "id": "ee3ad269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718900486991,
          "user_tz": 420,
          "elapsed": 14266,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "c6b07e3d-4b29-41a0-b507-99fd32218bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=b52d7b55b3405f1c6db2f698e968cefd2004048c8bc5485a2cee2a3d5eb501c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, rouge-score, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 rouge-score-0.1.2 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tensorflow datasets rouge-score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b8c7fc42",
      "metadata": {
        "id": "b8c7fc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "31e4dd9854484d6ea7ac0764f808dea5",
            "885c2db1a84e4734a59e5513f1dde635",
            "18a0179bdf4243cdb66fb9461a6f814e",
            "fc6ebff6f3804b969b9c940d06d5c3a2",
            "d84562b4fb1845478075f0ca3d205668",
            "4d94cab15ef74b72b664eea5b2bd0ae7",
            "88009677997b45c5bf2e962c683b65ba",
            "623256f1c36c4d21938c7de0c8d15d07",
            "d872315fa96c4f4bbdf27c33b690eb59",
            "ce952a5ba19847ddaa69667c81b0f27b",
            "595bca5aa86a4f158c7cb828c691cbd3",
            "70b13cf3fbf24626aa2b0b48e186c6d5",
            "d5cbb632296744beb723a9f4f39bd2c2",
            "ea6e83caa4904e439a9925aebf0680d1",
            "5c4c8709f7d048e59cab77255713e986",
            "11ecd62edbf44a0e918031966d7c6b0d",
            "f73cf0dcd46f4f63b41c117f57e5627a",
            "0ed8e46822eb40cfbcebbf61e9c059f3",
            "24db9c675d2a43cfa970d0cf44e05ecb",
            "5ad8d0f6f17b4c01b6eac225e4f0d390",
            "dd65b885bf1047328a967e0dab0afc64",
            "03ffd2b443f84155a371705240b504e1",
            "02463262bf554374a23448cd62e1f993",
            "5d87e930722442919f1c0441e799df67",
            "020c9aaf3c01481aaadac358faf9daff",
            "2dbee4fb0b254c5db555c5c79a436aec",
            "384a71f76a0946e8bdc7d1bb24c45ba6",
            "e7dc6407d66343c3aa67d0db9c5f7f17",
            "4216f6eb44fb4d78bc58f764fed1c110",
            "9827259c504c4301897dd7cc1923bb2c",
            "07af1e239aa74d13be34d8ee9dd9d388",
            "688ef5f86883404586c33deff7c2a273",
            "606e19c2a75546ea825ee735fffd54c1",
            "a170fd4713c54aa2990889aa55f23dc5",
            "55b051b69a7a434b818f57696e1611e7",
            "3515acf494de44539f9117cb415fcb53",
            "a9b0645f26f14233b7fda90ddd8b450c",
            "2f43a701db1e4b04b3b9de4b7f583bb0",
            "d89be46fedd7422abe83b4ef970846a9",
            "d8aaf7625a0f4cfba80ad07204b792b2",
            "e067b34b5c0443dfaf8af9d734f29cd4",
            "cea9f5d216ab45bcb299827cf4190f3e",
            "0fae685adc674b9abb3173b28df5ad9c",
            "d681877411d4489ba0d79502ea98354c",
            "93842324618f47d1a36cd30014389632",
            "9a957d27d46546ee921fc974b927b71b",
            "87b2efeba7104bfdb500fcf8908ff42f",
            "a3ef16dac105420685174d7fa73ee3f3",
            "e46cce8c43d14a7eb7fdefa3e36932ad",
            "83bd0a3279554b6998830e6ad17f1ba8",
            "9846ead990974e4ab8832a0c49a46591",
            "890db4de00c342f488d1d7667178bd52",
            "01f3548cf50f496e91040341b42a4ae7",
            "ba7edf8c6f26410aa25de99fc6c5dfe8",
            "02f28d7d802744428d136894bf53a672",
            "ce881c113983469799c623a138d73a4c",
            "831bbcb9b9b042feb5c9b1c0cba7aea5",
            "2fd2640d22024fc49bdafe672b849a5e",
            "0674070eb4a94f03ae03eb18c2c493a3",
            "afa2a93d337a47b3a6a77d94414e0a16",
            "c8c0cf3467834fa4931533ce70f4ac45",
            "ab78de4fe28242d8aea9b16b1b8389b6",
            "609381ebf9644cfd88b22493534654cb",
            "95feb67b75a549fc85f28c7e3f2f8dc6",
            "d7beca36927b4dda8fac13c4cf8b67bd",
            "191a33cfa1cf4cb993097bbd447c9cf5"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718900504166,
          "user_tz": 420,
          "elapsed": 17178,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "153455db-41ba-4e6d-d432-1cbb4ccf7cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31e4dd9854484d6ea7ac0764f808dea5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b13cf3fbf24626aa2b0b48e186c6d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02463262bf554374a23448cd62e1f993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a170fd4713c54aa2990889aa55f23dc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93842324618f47d1a36cd30014389632"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce881c113983469799c623a138d73a4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7958e52b",
      "metadata": {
        "id": "7958e52b"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5da0d76c",
      "metadata": {
        "id": "5da0d76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "c9884dd27f5e4a0388a14bd8a40bd993",
            "d774086eae4043bc8e7e3ece6894d5f1",
            "9a9a692a21234b20b730901b9cbbc842",
            "423294c242164c8e9ee45fa9d2381573",
            "4334e8104a2946db8bae6ced5bc3bf6f",
            "215280dcbc90480db61588d977ca9b87",
            "5a3957b7f3874315b1e5262ff2e7c934",
            "96be7c1d5c5a4387a7c13fc612fbf354",
            "81a8968b6eff41ba91cff5c984b5c46c",
            "736fd3fb7402499f97e74e8a85f07b82",
            "92ee72bfda914cb7ab328a237adfcddb",
            "dd78d1d5b5f544e2a02375dc723b0062",
            "df960bd8cc144e2396cf8c406a7fd481",
            "f3036516f17f41439561c986e151b609",
            "24493c39e2204d8ebabe1136ba1b10cb",
            "a2be9a75f5424b55a5f3bbf79f32c0ee",
            "69170656bc564728afd50ab004a2e563",
            "9872e67b6bcd471aa51040318feb358c",
            "750858d403084f59bf050cd9d41a2b2e",
            "6cd83de5674b4535bb0554e7291cbb35",
            "d0a9d1c56a7a46f5b133b28357dbf057",
            "36a03fff785a4eb384190f7255ee5109",
            "140a889cd1c54eca90433ef4ca012a3a",
            "3606c87ea56b462a8846259c79b9caab",
            "8eb4bb957d2947409967837b566cbf2d",
            "5a3d374bccba467685b8e4a72d1daac0",
            "076f3cb1debf42cb91079d52969416e7",
            "0b1043599bb745549def26050cfb3b8d",
            "3b9049def3724e9b8d24a38e3f176ed3",
            "939b8e86bf334c8ba9265b1e196c5118",
            "fddf09764cb042b0be31c97965f19070",
            "500a10f7b7d14401bdfbf8252416260e",
            "e1691ceda36b4f839fc990e7d2d1cb01",
            "f189f4cef0cd40ab8f351f47145b7a4c",
            "64fb4af62b96461f9d20d98b9e173c15",
            "c95ea0b07e014c3883b9c38d7113e7de",
            "4f666f4747874b46888c14d53fe3d860",
            "4714b31ba3644761801fc20a06940834",
            "6ca146e91ab84061a1845cfc4e5d16bc",
            "5c58653920b241819cb72b333fc5186e",
            "ddd9fad816c4476db561b1abaf3077ad",
            "a6718f6983a748e0b90db5f5484ea412",
            "81b8d0e9f3e240d98ff1de339ed8395a",
            "6f13d23115964f5883ed19a765a07ebe",
            "9ac63a67021f4f4595553f3b25fc6013",
            "d8f1afade5324cd59f0fdf67da298b44",
            "40861208632e4b1687ddd8eac5dbe92b",
            "813667a9d05742b9af816a5becac79c3",
            "37e1fa6b56174f7781f9db13e1bad82d",
            "13d9d56ec2214dc6b878f29f17b60501",
            "f8972863ab3d49f886d1638f0acd6f94",
            "e6c0b2645d7446e0a67ab4fd9197c05d",
            "08cab4db423c404ca116691909ae6384",
            "a8883612d2344b7bb33e08931bd1bbd8",
            "51debdff29624d419feae06e992a7454",
            "3d55b9ed09a0446183ec1363e4687c99",
            "8296149d442a4a5dab50f2665b4e00c5",
            "23dc6b437cfe49b98acd74023a29e541",
            "3838ba1a4db64658ba5f5e48a1dcfded",
            "be3d5faa34ca4e09b77a4dcfade06afa",
            "4f383b8a948544108dd62c4d0c50e339",
            "87892615bd86400794901a741400af36",
            "67a604bf018049c09f688760f7927585",
            "8cdae2dea72844c2be8b09b63d92bbf4",
            "621ac9be861d483aade9967d669c3229",
            "a1b1b546a7aa4e1d8f984cdb08edfe00",
            "dd6588df31964f60a095043a05a32a9f",
            "273bf487e26e4d1287c49efb0b201618",
            "aaf961dd3a1046d79c692f20fb7c19eb",
            "e26ee19faa08418790867e606d66101c",
            "defba6c22ccc465bbd66b6c169771a2b",
            "d5d397ac7ed146908e32a01742902c1d",
            "d1fecf7ab5d34e0693d7768f6d2f93e5",
            "4221d86b8fb242f9ba16e1885b98044d",
            "f89772a0bdd74f8bbd78381e11714a80",
            "c88bb69cc9954b499dffb47ee580842f",
            "6173c4cb376e4d08ad6714c93251d86c"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718900508688,
          "user_tz": 420,
          "elapsed": 4524,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "903c33bb-9778-4cc9-8d53-0d02d527b4ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9884dd27f5e4a0388a14bd8a40bd993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/237k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd78d1d5b5f544e2a02375dc723b0062"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "140a889cd1c54eca90433ef4ca012a3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/238k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f189f4cef0cd40ab8f351f47145b7a4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ac63a67021f4f4595553f3b25fc6013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d55b9ed09a0446183ec1363e4687c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd6588df31964f60a095043a05a32a9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'en': \"It was the asbestos in here, that's what did it!\", 'es': 'Fueron los asbestos aquí. ¡Eso es lo que ocurrió!'}}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Helsinki-NLP/opus-100 dataset\n",
        "dataset = load_dataset('Helsinki-NLP/opus-100', 'en-es')\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d149a8a4",
      "metadata": {
        "id": "d149a8a4"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "67e0d44f",
      "metadata": {
        "id": "67e0d44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "d70978c33ac74f2a9ef4b56d7bfcbcd0",
            "fd23391ec195474ab57523a0592412d9",
            "f3e799306315476ea86c016f02430e84",
            "f0cbe5322e7d499684704d3357a278ca",
            "46ab22fb004c44a48dbb2c0d8854afd1",
            "62f003191db14961899bec9faafb371a",
            "c2e4cf447b61444aa84d7e074225f000",
            "16660cb35c1f4be393ee9887d5ccaaac",
            "e7d24e26b2c94695bd87b782e6412afe",
            "07478fabaaea4698b67d67688f3c7386",
            "42f3b9ec724b42b4bd7113abc30b47b1",
            "a0001be02138406e9cd809737942e143",
            "e0a667e7f77740a2bb329118f0c8541a",
            "1b1ebcaf266f49fb85dd88302bc9dea7",
            "3979a0b0a4444b11abf80530aac15371",
            "a3f6bb0a1be54c40b00c2adc4c3ab512",
            "e0def0aed68f4bb5a424edcb9da64731",
            "1ed32d5e93d24429b308335e977cd802",
            "f5745127d6744f0490729d737b2b9221",
            "46e509d02d8b472095b682dae76e700d",
            "fdbf7f911b10409c81d3624cc299d152",
            "c712148c9d4347b2abdb9604f1dad9d9"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718900513904,
          "user_tz": 420,
          "elapsed": 5234,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "2b381c5f-ed7a-40fa-d471-6c6c2b59e890"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70978c33ac74f2a9ef4b56d7bfcbcd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0001be02138406e9cd809737942e143"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the dataset for input into the model\n",
        "def preprocess_data(examples):\n",
        "    inputs = [f'Translate from English to Spanish: {example[\"en\"]}' for example in examples['translation']]\n",
        "    targets = [example['es'] for example in examples['translation']]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
        "\n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # For decoder inputs\n",
        "    decoder_inputs = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "train_dataset = dataset['train'].select(range(30000)).map(preprocess_data, batched=True)\n",
        "test_dataset = dataset['test'].select(range(2000)).map(preprocess_data, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
        "    label_cols=['labels'],\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    collate_fn=None\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
        "    label_cols=['labels'],\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    collate_fn=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_layer(\"shared\").trainable = False\n",
        "model.get_layer(\"encoder\").trainable = False\n",
        "model.get_layer(\"decoder\").trainable = False"
      ],
      "metadata": {
        "id": "MFhIMBg8S8Zg",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718900514107,
          "user_tz": 420,
          "elapsed": 3,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        }
      },
      "id": "MFhIMBg8S8Zg",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41c704f3",
      "metadata": {
        "id": "41c704f3"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fc2058f0",
      "metadata": {
        "id": "fc2058f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718901249367,
          "user_tz": 420,
          "elapsed": 603871,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "eb82408f-3925-4e27-e618-81da9ceb6e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7b40e130f490> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7b40e130f490> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "469/469 [==============================] - 264s 362ms/step - loss: 48.3423\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 170s 362ms/step - loss: 47.3018\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 170s 362ms/step - loss: 46.3005\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7b402042a890>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import tf_keras\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b4fca62",
      "metadata": {
        "id": "0b4fca62"
      },
      "source": [
        "## Calculating ROUGE and BLEU Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eba84b07",
      "metadata": {
        "id": "eba84b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1718901494235,
          "user_tz": 420,
          "elapsed": 51726,
          "user": {
            "displayName": "Axel Sirota",
            "userId": "02089179879199828401"
          }
        },
        "outputId": "8e83afb0-7a00-481b-e7b4-e9ca5831536e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: - Estaba limpiando.\n",
            "Translation: Estaba limpiando.\n",
            "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
            "BLEU Score: 0.41252121275142684\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def translate(inputs):\n",
        "    outputs = model.generate(inputs[0][\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Function to calculate ROUGE and BLEU scores\n",
        "def calculate_scores(reference, hypothesis):\n",
        "    # Initialize scorers\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    bleu_smoothing = SmoothingFunction().method4\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_scores = rouge.score(reference, hypothesis)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference_tokens = [nltk.word_tokenize(reference)]\n",
        "    hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
        "    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=bleu_smoothing)\n",
        "\n",
        "    return rouge_scores, bleu_score\n",
        "\n",
        "# Evaluate translations and calculate scores\n",
        "batch = next(iter(test_dataset))\n",
        "translated_text = translate(batch)\n",
        "reference_text = tokenizer.decode(batch[1][0], skip_special_tokens=True)\n",
        "rouge_scores, bleu_score = calculate_scores(reference_text, translated_text)\n",
        "print(f\"Reference: {reference_text}\")\n",
        "print(f\"Translation: {translated_text}\")\n",
        "print(f\"ROUGE Scores: {rouge_scores}\")\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ca57a8",
      "metadata": {
        "id": "12ca57a8"
      },
      "source": [
        "## Conclusion\n",
        "In this notebook, we used the Flan-T5 model to perform translation from English to Spanish using the Helsinki-NLP/opus-100 dataset. We preprocessed the dataset, fine-tuned the model, and performed translations. We calculated ROUGE and BLEU scores to assess the quality of the translations. The results demonstrate the effectiveness of the Flan-T5 model for translation tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFWUmBn68IFG"
      },
      "id": "zFWUmBn68IFG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
